{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QtFZUh63Ie7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f13947-b404-4081-f2fd-7b00e7a947d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 4108, done.\u001b[K\n",
            "remote: Counting objects: 100% (4108/4108), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3109/3109), done.\u001b[K\n",
            "remote: Total 4108 (delta 1188), reused 2037 (delta 937), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (4108/4108), 45.36 MiB | 49.52 MiB/s, done.\n",
            "Resolving deltas: 100% (1188/1188), done.\n",
            "Processing /content/models/research\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: avro-python3 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.10.2)\n",
            "Requirement already satisfied: apache-beam in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.57.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (10.4.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (5.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (3.7.1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (3.0.10)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (21.6.0)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.16.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.0.8)\n",
            "Requirement already satisfied: lvis in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (0.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.11.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.0.3)\n",
            "Requirement already satisfied: tf-models-official>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.16.0)\n",
            "Requirement already satisfied: tensorflow_io in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (0.37.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.15.0)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.4.7)\n",
            "Requirement already satisfied: sacrebleu<=2.2.0 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.2.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2.10.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2024.5.15)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (1.25.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.4.6)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.8.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.2.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.6.14)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.10.0.84)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (6.0.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.1.99)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.2.2)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.9.6)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.16.1)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-text~=2.16.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.16.1)\n",
            "Collecting tensorflow~=2.16.1 (from tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Using cached tensorflow-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (590.6 MB)\n",
            "Requirement already satisfied: tf-keras>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->object-detection==0.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->object-detection==0.1) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->object-detection==0.1) (2024.1)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf-slim->object-detection==0.1) (1.4.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Requirement already satisfied: orjson<4,>=3.9.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (3.10.6)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (0.3.1.1)\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.9.4)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (0.19)\n",
            "Requirement already satisfied: grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.64.1)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (2.7.3)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (0.22.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (4.22.0)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: objsize<0.8.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (0.7.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (24.1)\n",
            "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (4.8.0)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (3.20.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.4.2)\n",
            "Requirement already satisfied: redis<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (5.0.7)\n",
            "Requirement already satisfied: requests!=2.32.*,<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (4.12.2)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (0.22.0)\n",
            "Requirement already satisfied: pyarrow<17.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (0.6)\n",
            "Requirement already satisfied: js2py<1,>=0.74 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (0.74)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (1.4.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (4.10.0.84)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (1.2.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (4.53.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.37.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_io->object-detection==0.1) (0.37.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.34.1)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam->object-detection==0.1) (5.2)\n",
            "Requirement already satisfied: pyjsparser>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1) (0.18.1)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2024.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (6.1.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo<5.0.0,>=3.8.0->apache-beam->object-detection==0.1) (2.6.1)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis<6,>=5.0.0->apache-beam->object-detection==0.1) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests!=2.32.*,<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests!=2.32.*,<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.7)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Using cached ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1) (2.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1) (1.14.1)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "Collecting keras (from object-detection==0.1)\n",
            "  Using cached keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->object-detection==0.1) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->object-detection==0.1) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->object-detection==0.1) (0.12.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.6.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (4.9)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->object-detection==0.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->object-detection==0.1) (2.18.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (8.1.7)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.1.5)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.10.2)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.5.1)\n",
            "Requirement already satisfied: etils[enp,epath,epy,etree]>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1) (0.43.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (6.4.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (3.19.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.63.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (5.3.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->object-detection==0.1) (0.1.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.5.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1) (3.0.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: docstring-parser~=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official>=2.5.1->object-detection==0.1) (2.1.5)\n",
            "Building wheels for collected packages: object-detection\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1697356 sha256=7954212c61a9f08122f18d942e3f92650a970d8c26e6003783faea4ec02f4fe8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l2jmj7d1/wheels/53/dd/70/2de274d6c443c69d367bd6a5606f95e5a6df61aacf1435ec0d\n",
            "Successfully built object-detection\n",
            "Installing collected packages: ml-dtypes, tensorboard, keras, tensorflow, object-detection\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "  Attempting uninstall: object-detection\n",
            "    Found existing installation: object-detection 0.1\n",
            "    Uninstalling object-detection-0.1:\n",
            "      Successfully uninstalled object-detection-0.1\n",
            "Successfully installed keras-3.4.1 ml-dtypes-0.3.2 object-detection-0.1 tensorboard-2.16.2 tensorflow-2.16.2\n"
          ]
        }
      ],
      "source": [
        "# uncomment the next line if you want to delete an existing models directory\n",
        "!rm -rf ./models/\n",
        "# clone the Tensorflow Model Garden\n",
        "!git clone --depth 1 https://github.com/tensorflow/models/\n",
        "\n",
        "# install the Object Detection API\n",
        "!cd models/research/ && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Iin5zBl7K9Ng",
        "outputId": "0a87ae6e-1a41-40b9-efc0-6ecdb4b9f43d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Using cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Using cached tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Installing collected packages: ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.3.2\n",
            "    Uninstalling ml-dtypes-0.3.2:\n",
            "      Successfully uninstalled ml-dtypes-0.3.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.16.2\n",
            "    Uninstalling tensorboard-2.16.2:\n",
            "      Successfully uninstalled tensorboard-2.16.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.16.2\n",
            "    Uninstalling tensorflow-2.16.2:\n",
            "      Successfully uninstalled tensorflow-2.16.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.15.0 which is incompatible.\n",
            "tf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tf-models-official 2.16.0 requires tensorflow~=2.16.1, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0\n"
          ]
        }
      ],
      "source": [
        "# need to use an older version of tensorflow\n",
        "!pip install tensorflow==2.15.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "\n",
        "import glob\n",
        "import imageio\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display, Javascript\n",
        "from IPython.display import Image as IPyImage\n",
        "import cv2\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "2UEgMD7cIwFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3abb7d7b-b1f4-43f4-8eca-71352836e499"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "099e9213-d4c3-4666-9dbb-460208862f44",
        "id": "IQNqZvD4xu-h"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd models/research/object_detection/\n",
        "# import the label map utility module\n",
        "from utils import label_map_util\n",
        "\n",
        "# import module for reading and updating configuration files.\n",
        "from utils import config_util\n",
        "\n",
        "# import module for visualization. use the alias `viz_utils`\n",
        "from utils import visualization_utils as viz_utils\n",
        "\n",
        "# import module for building the detection model\n",
        "from builders import model_builder\n",
        "### END CODE HERE ###\n",
        "\n",
        "# import module for utilities in Colab\n",
        "from object_detection.utils import colab_utils"
      ],
      "metadata": {
        "id": "sq_jgTmsOjpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe2ed19-dc33-4106-80ea-c2305ff8b681"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/object_detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "    \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "    Puts image into numpy array to feed into tensorflow graph.\n",
        "    Note that by convention we put it into a numpy array with shape\n",
        "    (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "    Args:\n",
        "    path: a file path.\n",
        "\n",
        "    Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    \"\"\"\n",
        "\n",
        "    # img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(img_data))\n",
        "\n",
        "    image_np = np.array(image)\n",
        "    image_np_3d = np.repeat(image_np[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "    (im_width, im_height) = image.size\n",
        "    return image_np_3d.reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "    # return np.array(image.getdata()).reshape(\n",
        "    #     (im_height, im_width)).astype(np.uint8)\n",
        "\n",
        "\n",
        "def plot_detections(image_np,\n",
        "                    boxes,\n",
        "                    classes,\n",
        "                    scores,\n",
        "                    category_index,\n",
        "                    figsize=(12, 16),\n",
        "                    image_name=None):\n",
        "    \"\"\"Wrapper function to visualize detections.\n",
        "\n",
        "    Args:\n",
        "    image_np: uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    boxes: a numpy array of shape [N, 4]\n",
        "    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
        "          and match the keys in the label map.\n",
        "    scores: a numpy array of shape [N] or None.  If scores=None, then\n",
        "          this function assumes that the boxes to be plotted are groundtruth\n",
        "          boxes and plot all boxes as black with no classes or scores.\n",
        "    category_index: a dict containing category dictionaries (each holding\n",
        "          category index `id` and category name `name`) keyed by category indices.\n",
        "    figsize: size for the figure.\n",
        "    image_name: a name for the image file.\n",
        "    \"\"\"\n",
        "\n",
        "    image_np_with_annotations = image_np.copy()\n",
        "\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np_with_annotations,\n",
        "        boxes,\n",
        "        classes,\n",
        "        scores,\n",
        "        category_index,\n",
        "        use_normalized_coordinates=True,\n",
        "        max_boxes_to_draw=500, # 20\n",
        "        min_score_thresh=0.3) # 0.8\n",
        "\n",
        "    if image_name:\n",
        "        plt.imsave(image_name, image_np_with_annotations)\n",
        "\n",
        "    else:\n",
        "        plt.imshow(image_np_with_annotations)"
      ],
      "metadata": {
        "id": "bxDdjtGiQZnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Visualize Data**"
      ],
      "metadata": {
        "id": "V5kOhFPOUx_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# image = cv2.imread('/content/drive/MyDrive/Cell_Mem_Dataset/PNGImages/1.png')\n",
        "train_image_dir = '/content/drive/MyDrive/Database/training_calcium_assay_jpg/6_clear_imgs'\n",
        "# declare an empty list\n",
        "train_images_np = []\n",
        "\n",
        "# run a for loop for each image, (1, Num_img+1)\n",
        "for i in range(1, 7):\n",
        "\n",
        "    # define the path (string) for each image\n",
        "    image_path = os.path.join(train_image_dir+'/'+str(i)+'.jpg')\n",
        "    print(image_path)\n",
        "\n",
        "    # load images into numpy arrays and append to a list\n",
        "    train_images_np.append(load_image_into_numpy_array(image_path))\n",
        "### END CODE HERE ###\n",
        "\n",
        "# configure plot settings via rcParams\n",
        "plt.rcParams['axes.grid'] = False\n",
        "plt.rcParams['xtick.labelsize'] = False\n",
        "plt.rcParams['ytick.labelsize'] = False\n",
        "plt.rcParams['xtick.top'] = False\n",
        "plt.rcParams['xtick.bottom'] = False\n",
        "plt.rcParams['ytick.left'] = False\n",
        "plt.rcParams['ytick.right'] = False\n",
        "plt.rcParams['figure.figsize'] = [14, 7]\n",
        "\n",
        "# plot images\n",
        "for idx, train_image_np in enumerate(train_images_np):\n",
        "    plt.subplot(1, 6, idx+1)\n",
        "    plt.imshow(train_image_np)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C-j46TpEH8YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare Data for training**"
      ],
      "metadata": {
        "id": "PM-Ya4mHVHqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Draw my own ground truth box"
      ],
      "metadata": {
        "id": "t8BJtkX8N-H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of ground truth boxes\n",
        "gt_boxes = []\n",
        "# Option 1: draw your own ground truth boxes\n",
        "\n",
        "# annotate the training images\n",
        "colab_utils.annotate(train_images_np, box_storage_pointer=gt_boxes)\n",
        "\n",
        "# Option 1: draw your own ground truth boxes\n",
        "# TEST CODE:\n",
        "try:\n",
        "  assert(len(gt_boxes) == 5), \"Warning: gt_boxes is empty. Did you click `submit`?\"\n",
        "\n",
        "except AssertionError as e:\n",
        "  print(e)\n",
        "\n",
        "# checks if there are boxes for all 5 images\n",
        "for gt_box in gt_boxes:\n",
        "    try:\n",
        "      assert(gt_box is not None), \"There are less than 5 sets of box coordinates. \" \\\n",
        "                                  \"Please re-run the cell above to draw the boxes again.\\n\" \\\n",
        "                                  \"Alternatively, you can run the next cell to load pre-determined \" \\\n",
        "                                  \"ground truth boxes.\"\n",
        "\n",
        "    except AssertionError as e:\n",
        "        print(e)\n",
        "        break\n",
        "\n",
        "\n",
        "# ref_gt_boxes = [\n",
        "#         np.array([[0.27333333, 0.41500586, 0.74333333, 0.57678781]]),\n",
        "#         np.array([[0.29833333, 0.45955451, 0.75666667, 0.61078546]]),\n",
        "#         np.array([[0.40833333, 0.18288394, 0.945, 0.34818288]]),\n",
        "#         np.array([[0.16166667, 0.61899179, 0.8, 0.91910903]]),\n",
        "#         np.array([[0.28833333, 0.12543962, 0.835, 0.35052755]]),\n",
        "#       ]\n",
        "\n",
        "# for gt_box, ref_gt_box in zip(gt_boxes, ref_gt_boxes):\n",
        "#     try:\n",
        "#       assert(np.allclose(gt_box, ref_gt_box, atol=0.04)), \"One of the boxes is too big or too small. \" \\\n",
        "#                                                           \"Please re-draw and make the box tighter around the cell.\"\n",
        "\n",
        "#     except AssertionError as e:\n",
        "#       print(e)\n",
        "#       break"
      ],
      "metadata": {
        "id": "PSESnzR8UuHw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "9f492bb8-a6b5-41c2-cbce-ee9972379c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "              async function load_image(imgs, callbackId) {\n",
              "                  //init organizational elements\n",
              "                  const div = document.createElement('div');\n",
              "                  var image_cont = document.createElement('div');\n",
              "                  var errorlog = document.createElement('div');\n",
              "                  var crosshair_h = document.createElement('div');\n",
              "                  crosshair_h.style.position = \"absolute\";\n",
              "                  crosshair_h.style.backgroundColor = \"transparent\";\n",
              "                  crosshair_h.style.width = \"100%\";\n",
              "                  crosshair_h.style.height = \"0px\";\n",
              "                  crosshair_h.style.zIndex = 9998;\n",
              "                  crosshair_h.style.borderStyle = \"dotted\";\n",
              "                  crosshair_h.style.borderWidth = \"2px\";\n",
              "                  crosshair_h.style.borderColor = \"rgba(255, 0, 0, 0.75)\";\n",
              "                  crosshair_h.style.cursor = \"crosshair\";\n",
              "                  var crosshair_v = document.createElement('div');\n",
              "                  crosshair_v.style.position = \"absolute\";\n",
              "                  crosshair_v.style.backgroundColor = \"transparent\";\n",
              "                  crosshair_v.style.width = \"0px\";\n",
              "                  crosshair_v.style.height = \"100%\";\n",
              "                  crosshair_v.style.zIndex = 9999;\n",
              "                  crosshair_v.style.top = \"0px\";\n",
              "                  crosshair_v.style.borderStyle = \"dotted\";\n",
              "                  crosshair_v.style.borderWidth = \"2px\";\n",
              "                  crosshair_v.style.borderColor = \"rgba(255, 0, 0, 0.75)\";\n",
              "                  crosshair_v.style.cursor = \"crosshair\";\n",
              "                  crosshair_v.style.marginTop = \"23px\";\n",
              "                  var brdiv = document.createElement('br');\n",
              "\n",
              "\n",
              "                  //init control elements\n",
              "                  var next = document.createElement('button');\n",
              "                  var prev = document.createElement('button');\n",
              "                  var submit = document.createElement('button');\n",
              "                  var deleteButton = document.createElement('button');\n",
              "                  var deleteAllbutton = document.createElement('button');\n",
              "\n",
              "                  //init image containers\n",
              "                  var image = new Image();\n",
              "                  var canvas_img = document.createElement('canvas');\n",
              "                  var ctx = canvas_img.getContext(\"2d\");\n",
              "                  canvas_img.style.cursor = \"crosshair\";\n",
              "                  canvas_img.setAttribute('draggable', false);\n",
              "                  crosshair_v.setAttribute('draggable', false);\n",
              "                  crosshair_h.setAttribute('draggable', false);\n",
              "\n",
              "                  // bounding box containers\n",
              "                  const height = 600\n",
              "                  var allBoundingBoxes = [];\n",
              "                  var curr_image = 0\n",
              "                  var im_height = 0;\n",
              "                  var im_width = 0;\n",
              "\n",
              "                  //initialize bounding boxes\n",
              "                  for (var i = 0; i < imgs.length; i++) {\n",
              "                    allBoundingBoxes[i] = [];\n",
              "                  }\n",
              "                  //initialize image view\n",
              "                  errorlog.id = 'errorlog';\n",
              "                  image.style.display = 'block';\n",
              "                  image.setAttribute('draggable', false);\n",
              "\n",
              "                  //load the first image\n",
              "                  img = imgs[curr_image];\n",
              "                  image.src = \"data:image/png;base64,\" + img;\n",
              "                  image.onload = function() {\n",
              "                      // normalize display height and canvas\n",
              "                      image.height = height;\n",
              "                      image_cont.height = canvas_img.height = image.height;\n",
              "                      image_cont.width = canvas_img.width = image.naturalWidth;\n",
              "                      crosshair_v.style.height = image_cont.height + \"px\";\n",
              "                      crosshair_h.style.width = image_cont.width + \"px\";\n",
              "\n",
              "                      // draw the new image\n",
              "                      ctx.drawImage(image, 0, 0, image.naturalWidth, image.naturalHeight, 0, 0,  canvas_img.width,  canvas_img.height);\n",
              "\n",
              "                  };\n",
              "\n",
              "                  // move to next image in array\n",
              "                  next.textContent = \"next image\";\n",
              "                  next.onclick = function(){\n",
              "                      if (curr_image < imgs.length - 1){\n",
              "                          // clear canvas and load new image\n",
              "                          curr_image += 1;\n",
              "                          errorlog.innerHTML = \"\";\n",
              "                      }\n",
              "                      else{\n",
              "                          errorlog.innerHTML = \"All images completed!!\";\n",
              "                      }\n",
              "                      resetcanvas();\n",
              "                  }\n",
              "\n",
              "                  //move forward through list of images\n",
              "                  prev.textContent = \"prev image\"\n",
              "                  prev.onclick = function(){\n",
              "                      if (curr_image > 0){\n",
              "                          // clear canvas and load new image\n",
              "                          curr_image -= 1;\n",
              "                          errorlog.innerHTML = \"\";\n",
              "                      }\n",
              "                      else{\n",
              "                          errorlog.innerHTML = \"at the beginning\";\n",
              "                      }\n",
              "                      resetcanvas();\n",
              "                  }\n",
              "                  // on delete, deletes the last bounding box\n",
              "                  deleteButton.textContent = \"undo bbox\";\n",
              "                  deleteButton.onclick = function(){\n",
              "                    boundingBoxes.pop();\n",
              "                    ctx.clearRect(0, 0, canvas_img.width, canvas_img.height);\n",
              "                    image.src = \"data:image/png;base64,\" + img;\n",
              "                    image.onload = function() {\n",
              "                        ctx.drawImage(image, 0, 0, image.naturalWidth, image.naturalHeight, 0, 0,  canvas_img.width,  canvas_img.height);\n",
              "                        boundingBoxes.map(r => {drawRect(r)});\n",
              "                    };\n",
              "                  }\n",
              "                  // on all delete, deletes all of the bounding box\n",
              "                  deleteAllbutton.textContent = \"delete all\"\n",
              "                  deleteAllbutton.onclick = function(){\n",
              "                    boundingBoxes = [];\n",
              "                    ctx.clearRect(0, 0, canvas_img.width, canvas_img.height);\n",
              "                    image.src = \"data:image/png;base64,\" + img;\n",
              "                    image.onload = function() {\n",
              "                        ctx.drawImage(image, 0, 0, image.naturalWidth, image.naturalHeight, 0, 0,  canvas_img.width,  canvas_img.height);\n",
              "                        //boundingBoxes.map(r => {drawRect(r)});\n",
              "                    };\n",
              "                  }\n",
              "\n",
              "                  // on submit, send the boxes to display\n",
              "                  submit.textContent = \"submit\";\n",
              "                  submit.onclick = function(){\n",
              "                    errorlog.innerHTML = \"\";\n",
              "\n",
              "                    // send box data to callback fucntion\n",
              "                    google.colab.kernel.invokeFunction(callbackId, [allBoundingBoxes], {});\n",
              "                  }\n",
              "\n",
              "                // init template for annotations\n",
              "                const annotation = {\n",
              "                      x: 0,\n",
              "                      y: 0,\n",
              "                      w: 0,\n",
              "                      h: 0,\n",
              "                };\n",
              "\n",
              "                // the array of all rectangles\n",
              "                let boundingBoxes = allBoundingBoxes[curr_image];\n",
              "\n",
              "                // the actual rectangle, the one that is being drawn\n",
              "                let o = {};\n",
              "\n",
              "                // a variable to store the mouse position\n",
              "                let m = {},\n",
              "\n",
              "                // a variable to store the point where you begin to draw the\n",
              "                // rectangle\n",
              "                start = {};\n",
              "\n",
              "                // a boolean variable to store the drawing state\n",
              "                let isDrawing = false;\n",
              "                var elem = null;\n",
              "\n",
              "                function handleMouseDown(e) {\n",
              "                  // on mouse click set change the cursor and start tracking the mouse position\n",
              "                  start = oMousePos(canvas_img, e);\n",
              "\n",
              "                  // configure is drawing to true\n",
              "                  isDrawing = true;\n",
              "                }\n",
              "\n",
              "                function handleMouseMove(e) {\n",
              "                    // move crosshairs, but only within the bounds of the canvas\n",
              "                    if (document.elementsFromPoint(e.pageX, e.pageY).includes(canvas_img)) {\n",
              "                      crosshair_h.style.top = e.pageY + \"px\";\n",
              "                      crosshair_v.style.left = e.pageX + \"px\";\n",
              "                    }\n",
              "\n",
              "                    // move the bounding box\n",
              "                    if(isDrawing){\n",
              "                      m = oMousePos(canvas_img, e);\n",
              "                      draw();\n",
              "                    }\n",
              "                }\n",
              "\n",
              "                function handleMouseUp(e) {\n",
              "                    if (isDrawing) {\n",
              "                        // on mouse release, push a bounding box to array and draw all boxes\n",
              "                        isDrawing = false;\n",
              "\n",
              "                        const box = Object.create(annotation);\n",
              "\n",
              "                        // calculate the position of the rectangle\n",
              "                        if (o.w > 0){\n",
              "                          box.x = o.x;\n",
              "                        }\n",
              "                        else{\n",
              "                          box.x = o.x + o.w;\n",
              "                        }\n",
              "                        if (o.h > 0){\n",
              "                          box.y = o.y;\n",
              "                        }\n",
              "                        else{\n",
              "                          box.y = o.y + o.h;\n",
              "                        }\n",
              "                        box.w = Math.abs(o.w);\n",
              "                        box.h = Math.abs(o.h);\n",
              "\n",
              "                        // add the bounding box to the image\n",
              "                        boundingBoxes.push(box);\n",
              "                        draw();\n",
              "                    }\n",
              "                }\n",
              "\n",
              "                function draw() {\n",
              "                    o.x = (start.x)/image.width;  // start position of x\n",
              "                    o.y = (start.y)/image.height;  // start position of y\n",
              "                    o.w = (m.x - start.x)/image.width;  // width\n",
              "                    o.h = (m.y - start.y)/image.height;  // height\n",
              "\n",
              "                    ctx.clearRect(0, 0, canvas_img.width, canvas_img.height);\n",
              "                    ctx.drawImage(image, 0, 0, image.naturalWidth, image.naturalHeight, 0, 0,  canvas_img.width,  canvas_img.height);\n",
              "                    // draw all the rectangles saved in the rectsRy\n",
              "                    boundingBoxes.map(r => {drawRect(r)});\n",
              "                    // draw the actual rectangle\n",
              "                    drawRect(o);\n",
              "                }\n",
              "\n",
              "                // add the handlers needed for dragging\n",
              "                crosshair_h.addEventListener(\"mousedown\", handleMouseDown);\n",
              "                crosshair_v.addEventListener(\"mousedown\", handleMouseDown);\n",
              "                document.addEventListener(\"mousemove\", handleMouseMove);\n",
              "                document.addEventListener(\"mouseup\", handleMouseUp);\n",
              "\n",
              "\n",
              "                function resetcanvas(){\n",
              "                    // clear canvas\n",
              "                    ctx.clearRect(0, 0, canvas_img.width, canvas_img.height);\n",
              "                    img = imgs[curr_image]\n",
              "                    image.src = \"data:image/png;base64,\" + img;\n",
              "\n",
              "                    // onload init new canvas and display image\n",
              "                    image.onload = function() {\n",
              "                        // normalize display height and canvas\n",
              "                        image.height = height;\n",
              "                        image_cont.height = canvas_img.height = image.height;\n",
              "                        image_cont.width = canvas_img.width = image.naturalWidth;\n",
              "                        crosshair_v.style.height = image_cont.height + \"px\";\n",
              "                        crosshair_h.style.width = image_cont.width + \"px\";\n",
              "\n",
              "                        // draw the new image\n",
              "                        ctx.drawImage(image, 0, 0, image.naturalWidth, image.naturalHeight, 0, 0,  canvas_img.width,  canvas_img.height);\n",
              "\n",
              "                        // draw bounding boxes\n",
              "                        boundingBoxes = allBoundingBoxes[curr_image];\n",
              "                        boundingBoxes.map(r => {drawRect(r)});\n",
              "                    };\n",
              "                }\n",
              "\n",
              "                function drawRect(o){\n",
              "                    // draw a predefined rectangle\n",
              "                    ctx.strokeStyle = \"red\";\n",
              "                    ctx.lineWidth = 2;\n",
              "                    ctx.beginPath(o);\n",
              "                    ctx.rect(o.x * image.width, o.y * image.height, o.w * image.width, o.h * image.height);\n",
              "                    ctx.stroke();\n",
              "                }\n",
              "\n",
              "                // Function to detect the mouse position\n",
              "                function oMousePos(canvas_img, evt) {\n",
              "                  let ClientRect = canvas_img.getBoundingClientRect();\n",
              "                    return {\n",
              "                      x: evt.clientX - ClientRect.left,\n",
              "                      y: evt.clientY - ClientRect.top\n",
              "                    };\n",
              "                }\n",
              "\n",
              "\n",
              "                //configure colab output display\n",
              "                google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "                //build the html document that will be seen in output\n",
              "                div.appendChild(document.createElement('br'))\n",
              "                div.appendChild(image_cont)\n",
              "                image_cont.appendChild(canvas_img)\n",
              "                image_cont.appendChild(crosshair_h)\n",
              "                image_cont.appendChild(crosshair_v)\n",
              "                div.appendChild(document.createElement('br'))\n",
              "                div.appendChild(errorlog)\n",
              "                div.appendChild(prev)\n",
              "                div.appendChild(next)\n",
              "                div.appendChild(deleteButton)\n",
              "                div.appendChild(deleteAllbutton)\n",
              "                div.appendChild(document.createElement('br'))\n",
              "                div.appendChild(brdiv)\n",
              "                div.appendChild(submit)\n",
              "                document.querySelector(\"#output-area\").appendChild(div);\n",
              "                return\n",
              "            }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: gt_boxes is empty. Did you click `submit`?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"670b2d6c-a806-11ee-bcdf-0242ac1c000c\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_94d302281e"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"670b7c36-a806-11ee-bcdf-0242ac1c000c\"] = document.querySelector(\"#errorlog\");\n",
              "//# sourceURL=js_5a7fbf3c1b"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"670bc51a-a806-11ee-bcdf-0242ac1c000c\"] = google.colab.output.setActiveOutputArea(window[\"670b7c36-a806-11ee-bcdf-0242ac1c000c\"]);\n",
              "//# sourceURL=js_8061bea0f3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'--boxes array populated--'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"670c4166-a806-11ee-bcdf-0242ac1c000c\"] = google.colab.output.setActiveOutputArea(window[\"670b2d6c-a806-11ee-bcdf-0242ac1c000c\"]);\n",
              "//# sourceURL=js_6f1418a13c"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"809bf3e2-a806-11ee-bcdf-0242ac1c000c\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_065f8f99af"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"809c3d48-a806-11ee-bcdf-0242ac1c000c\"] = document.querySelector(\"#errorlog\");\n",
              "//# sourceURL=js_2b9b4db130"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"809c7bd2-a806-11ee-bcdf-0242ac1c000c\"] = google.colab.output.setActiveOutputArea(window[\"809c3d48-a806-11ee-bcdf-0242ac1c000c\"]);\n",
              "//# sourceURL=js_1a5d56c619"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'--boxes array populated--'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"809ce3d8-a806-11ee-bcdf-0242ac1c000c\"] = google.colab.output.setActiveOutputArea(window[\"809bf3e2-a806-11ee-bcdf-0242ac1c000c\"]);\n",
              "//# sourceURL=js_bf0d9f9d46"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print and save the coordinates of your ground truth boxes\n",
        "from numpy import savetxt\n",
        "i = 1\n",
        "for gt_box in gt_boxes:\n",
        "  print(gt_box)\n",
        "  # save to csv file\n",
        "  save_path = os.path.join(train_image_dir+'/'+'boxes_all'+'/'+str(i)+'.csv')\n",
        "  savetxt(save_path, gt_box, delimiter=' ')\n",
        "  i += 1"
      ],
      "metadata": {
        "id": "ewQMYHO8UiRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(gt_box)"
      ],
      "metadata": {
        "id": "_nAOhg6qRqqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d952843-0d93-45b7-9efe-452e17acbae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Given ground truth boxes"
      ],
      "metadata": {
        "id": "wE8k4hn5SmeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import loadtxt\n",
        "# load saved boxes\n",
        "gt_boxes = []\n",
        "for i in range(1,7):\n",
        "  save_path = os.path.join(train_image_dir+'/'+'boxes_all'+'/'+str(i)+'.csv')\n",
        "  # gt_box = np.array(loadtxt(save_path, delimiter=' '))\n",
        "  gt_box = loadtxt(save_path, delimiter=' ')\n",
        "  gt_boxes.append(gt_box)\n",
        "  i += 1\n",
        "\n",
        "  print(gt_boxes)"
      ],
      "metadata": {
        "id": "emcSw6YZSMlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define category index dictionary**"
      ],
      "metadata": {
        "id": "CYsWUVVwVVcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the cell class ID\n",
        "cell_class_id = 1\n",
        "\n",
        "# define a dictionary describing the cell class\n",
        "category_index = {cell_class_id :\n",
        "                 {'id'  : cell_class_id,\n",
        "                  'name': 'cell'}\n",
        "                 }\n",
        "\n",
        "# Specify the number of classes that the model will predict\n",
        "num_classes = 1\n",
        "\n",
        "# TEST CODE:\n",
        "\n",
        "print(category_index[cell_class_id])"
      ],
      "metadata": {
        "id": "wmxY8ZKUVGGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac02a6cb-bff0-4e14-8390-6e961430a934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 1, 'name': 'cell'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preprocessing**"
      ],
      "metadata": {
        "id": "IgiyLdi4WS8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the class labels to one-hot representations\n",
        "convert everything (i.e. train images, gt boxes and class labels) to tensors."
      ],
      "metadata": {
        "id": "ljqmhVlfWiDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The `label_id_offset` here shifts all classes by a certain number of indices;\n",
        "# we do this here so that the model receives one-hot labels where non-background\n",
        "# classes start counting at the zeroth index.  This is ordinarily just handled\n",
        "# automatically in our training binaries, but we need to reproduce it here.\n",
        "\n",
        "label_id_offset = 1\n",
        "train_image_tensors = []\n",
        "\n",
        "# lists containing the one-hot encoded classes and ground truth boxes\n",
        "gt_classes_one_hot_tensors = []\n",
        "gt_box_tensors = []\n",
        "\n",
        "for (train_image_np, gt_box_np) in zip(train_images_np, gt_boxes):\n",
        "\n",
        "    # convert training image to tensor, add batch dimension, and add to list\n",
        "    train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n",
        "        train_image_np, dtype=tf.float32), axis=0))\n",
        "\n",
        "    # convert numpy array to tensor, then add to list\n",
        "    gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
        "\n",
        "    # apply offset to to have zero-indexed ground truth classes\n",
        "    zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n",
        "        np.ones(shape=[gt_box_np.shape[0]], dtype=np.int32) - label_id_offset)\n",
        "\n",
        "    # do one-hot encoding to ground truth classes\n",
        "    gt_classes_one_hot_tensors.append(tf.one_hot(\n",
        "        zero_indexed_groundtruth_classes, num_classes))\n",
        "\n",
        "print('Done prepping data.')"
      ],
      "metadata": {
        "id": "eiNYqvRoV15I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5863a4ed-30cf-42bf-b34f-ac24f323cacb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done prepping data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# give boxes a score of 100%\n",
        "dummy_scores = np.array([1.0], dtype=np.float32)\n",
        "\n",
        "# define the figure size\n",
        "plt.figure(figsize=(30, 15))\n",
        "\n",
        "# use the `plot_detections()` utility function to draw the ground truth boxes\n",
        "for idx in range(5):\n",
        "    plt.subplot(2, 4, idx+1)\n",
        "    plot_detections(\n",
        "      train_images_np[idx],\n",
        "      gt_boxes[idx],\n",
        "      # np.ones(shape=[gt_boxes[idx].shape[0]], dtype=np.int32),\n",
        "      # dummy_scores, category_index)\n",
        "      np.ones(shape=[gt_boxes[idx].shape[0]], dtype=np.int32),\n",
        "      None, category_index)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VanZL9bXWjdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download the checkpoint containing the pre-trained weights**"
      ],
      "metadata": {
        "id": "lX76yHpD1wdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE ###\n",
        "# Download the SSD Resnet 50 version 1, 640x640 checkpoint\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "!mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint test_data/\n",
        "\n",
        "# untar (decompress) the tar file\n",
        "None\n",
        "\n",
        "# copy the checkpoint to the test_data folder models/research/object_detection/test_data/\n",
        "None\n",
        "\n",
        "### END CODE HERE"
      ],
      "metadata": {
        "id": "dqHyVuNL1shU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure the Model**"
      ],
      "metadata": {
        "id": "EREId0pA1_s8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "from utils.config_util import get_configs_from_pipeline_file\n",
        "# define the path to the .config file for ssd resnet 50 v1 640x640\n",
        "pipeline_config = \"/content/models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config\"\n",
        "# Load the configuration file into a dictionary\n",
        "configs = get_configs_from_pipeline_file(pipeline_config, config_override=None)\n",
        "# See what configs looks like\n",
        "configs"
      ],
      "metadata": {
        "id": "dkMw23YW1-9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get model configuration"
      ],
      "metadata": {
        "id": "1SgHkA1_2P7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the object stored at the key 'model' of the configs dictionary\n",
        "model_config = configs['model']\n",
        "\n",
        "# see what model_config looks like\n",
        "model_config"
      ],
      "metadata": {
        "id": "bdgOszjr2R0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Modify model_config"
      ],
      "metadata": {
        "id": "Oz-QzmVb2hzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the number of classes from its default of 90\n",
        "model_config.ssd.num_classes = num_classes\n",
        "# Freeze batch normalization\n",
        "model_config.ssd.freeze_batchnorm = True\n",
        "\n",
        "model_config.ssd.post_processing.batch_non_max_suppression.max_detections_per_class = 1000\n",
        "model_config.ssd.post_processing.batch_non_max_suppression.max_total_detections = 1000\n",
        "model_config.ssd.post_processing.batch_non_max_suppression.iou_threshold = 0.3\n",
        "\n",
        "# model_config.ssd.box_coder.faster_rcnn_box_coder.y_scale = 3\n",
        "# model_config.ssd.box_coder.faster_rcnn_box_coder.x_scale = 3\n",
        "# model_config.ssd.box_coder.faster_rcnn_box_coder.height_scale = 2\n",
        "# model_config.ssd.box_coder.faster_rcnn_box_coder.height_scale = 2\n",
        "\n",
        "\n",
        "# See what model_config now looks like after you've customized it!\n",
        "model_config"
      ],
      "metadata": {
        "id": "vrB94i4h2fNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modify train_config"
      ],
      "metadata": {
        "id": "29FZL0VsVDcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_config = configs['train_config']\n",
        "train_config.data_augmentation_options[1].random_crop_image.max_aspect_ratio = 1.5\n",
        "train_config.max_number_of_boxes = 1000\n",
        "train_config"
      ],
      "metadata": {
        "id": "gMh8x3p9VJrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs"
      ],
      "metadata": {
        "id": "SiP-DBx5qGYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the model**"
      ],
      "metadata": {
        "id": "aXp_1i3D2tA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the custom model"
      ],
      "metadata": {
        "id": "z1SrTBL_3Bt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "detection_model = model_builder.build(\n",
        "      model_config=model_config, is_training=True)\n",
        "\n",
        "print(type(detection_model))"
      ],
      "metadata": {
        "id": "pRQ6Xxi52xp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inspect the detection_model**"
      ],
      "metadata": {
        "id": "nlEjJZHHrLUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to check the type of detection_model\n",
        "detection_model"
      ],
      "metadata": {
        "id": "9FIE9fA-3L_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vars(detection_model)"
      ],
      "metadata": {
        "id": "Tym8u8-J3OG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view the type of _box_predictor\n",
        "detection_model._box_predictor"
      ],
      "metadata": {
        "id": "EC4bXmS23U7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vars(detection_model._box_predictor)"
      ],
      "metadata": {
        "id": "4uRLKIri3Xcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Checkpoints for the box predictor**"
      ],
      "metadata": {
        "id": "D9wHBh6y3jYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_box_predictor_checkpoint = tf.compat.v2.train.Checkpoint(\n",
        "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
        "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
        "    )\n",
        "\n",
        "# Check the datatype of this checkpoint\n",
        "type(tmp_box_predictor_checkpoint)\n",
        "\n",
        "# Check the variables of this checkpoint\n",
        "vars(tmp_box_predictor_checkpoint)"
      ],
      "metadata": {
        "id": "u3Xj1vMU3pfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the temporary model checkpoint**"
      ],
      "metadata": {
        "id": "taJyKzKr39vL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_model_checkpoint = tf.compat.v2.train.Checkpoint(\n",
        "          _feature_extractor=detection_model._feature_extractor,\n",
        "          _box_predictor=tmp_box_predictor_checkpoint)\n",
        "\n",
        "tmp_model_checkpoint = tf.compat.v2.train.Checkpoint(model=tmp_model_checkpoint)\n",
        "\n",
        "# Check the datatype of this checkpoint\n",
        "type(tmp_model_checkpoint)\n",
        "\n",
        "# Check the vars of this checkpoint\n",
        "vars(tmp_model_checkpoint)"
      ],
      "metadata": {
        "id": "wAle8YNz37E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restore the checkpoint**"
      ],
      "metadata": {
        "id": "9cL4L41G4P3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = '/content/models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
        "\n",
        "# Define a checkpoint that sets `model= None\n",
        "checkpoint =tf.train.Checkpoint(model=tmp_model_checkpoint)\n",
        "\n",
        "# Restore the checkpoint to the checkpoint path\n",
        "tmp_model_checkpoint.restore(checkpoint_path).expect_partial()"
      ],
      "metadata": {
        "id": "J7Ho3H-q4Tbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run a dummy image to generate the model variables**"
      ],
      "metadata": {
        "id": "XgMH2mtb4iy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the detection model's `preprocess()` method and pass a dummy image\n",
        "tmp_image, tmp_shapes =  detection_model.preprocess(tf.zeros([1, 968, 548, 3]))\n",
        "\n",
        "# run a prediction with the preprocessed image and shapes\n",
        "tmp_prediction_dict = detection_model.predict(tmp_image, tmp_shapes)\n",
        "\n",
        "# postprocess the predictions into final detections\n",
        "tmp_detections = detection_model.postprocess(tmp_prediction_dict, tmp_shapes)\n",
        "print('Weights restored!')"
      ],
      "metadata": {
        "id": "kdPVmF-94lKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Code:\n",
        "assert len(detection_model.trainable_variables) > 0, \"Please pass in a dummy image to create the trainable variables.\"\n",
        "\n",
        "print(detection_model.weights[0].shape)\n",
        "print(detection_model.weights[231].shape)\n",
        "print(detection_model.weights[462].shape)"
      ],
      "metadata": {
        "id": "UedRKM4D4tUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Eager mode custom training loop**"
      ],
      "metadata": {
        "id": "NBH2kHvj45SM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set training hyperparameters**"
      ],
      "metadata": {
        "id": "H4RXyg85475g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.set_learning_phase(True)\n",
        "\n",
        "# set the batch_size = Num_img-1\n",
        "batch_size =5\n",
        "\n",
        "# set the number of batches\n",
        "num_batches = 100\n",
        "\n",
        "# Set the learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "# set the optimizer and pass in the learning_rate\n",
        "optimizer = tf.keras.optimizers.SGD(\n",
        "    learning_rate=learning_rate, momentum=0.9)"
      ],
      "metadata": {
        "id": "W9ikJ5-M4-pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choose the layers to fine-tune**"
      ],
      "metadata": {
        "id": "BMJQe_5y5E5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i,v in enumerate(detection_model.trainable_variables):\n",
        "    print(f\"i: {i} \\t name: {v.name} \\t shape:{v.shape} \\t dtype={v.dtype}\")"
      ],
      "metadata": {
        "id": "_gF57QyF5JQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select the prediction layer variables**"
      ],
      "metadata": {
        "id": "ifzJp2dq5TaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a list that contains the layers that you wish to fine tune\n",
        "to_fine_tune = []\n",
        "for v in detection_model.trainable_variables:\n",
        "  if v.name.startswith('WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead') or v.name.startswith('WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead') :\n",
        "    to_fine_tune.append(v)"
      ],
      "metadata": {
        "id": "-3oO-BJS5RAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Code:\n",
        "\n",
        "print(to_fine_tune[0].name)\n",
        "print(to_fine_tune[2].name)"
      ],
      "metadata": {
        "id": "wsEUU8R75YEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train your model**"
      ],
      "metadata": {
        "id": "4rVrRqAc5dFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of your training images\n",
        "g_images_list = train_image_tensors[0:2]"
      ],
      "metadata": {
        "id": "0aoA61-L5dqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(g_images_list[0])"
      ],
      "metadata": {
        "id": "24swjAgI89tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use .preprocess to preprocess an image\n",
        "g_preprocessed_image = detection_model.preprocess(g_images_list[0])\n",
        "print(f\"g_preprocessed_image type: {type(g_preprocessed_image)}\")\n",
        "print(f\"g_preprocessed_image length: {len(g_preprocessed_image)}\")\n",
        "print(f\"index 0 has the preprocessed image of shape {g_preprocessed_image[0].shape}\")\n",
        "print(f\"index 1 has information about the image's true shape excluding padding: {g_preprocessed_image[1]}\")"
      ],
      "metadata": {
        "id": "Ww-S8lqi5jzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_image_list = []\n",
        "true_shape_list = []\n",
        "\n",
        "for img in g_images_list:\n",
        "    processed_img, true_shape = detection_model.preprocess(img)\n",
        "    preprocessed_image_list.append(processed_img)\n",
        "    true_shape_list.append(true_shape)\n",
        "\n",
        "print(f\"preprocessed_image_list is of type {type(preprocessed_image_list)}\")\n",
        "print(f\"preprocessed_image_list has length {len(preprocessed_image_list)}\")\n",
        "print()\n",
        "print(f\"true_shape_list is of type {type(true_shape_list)}\")\n",
        "print(f\"true_shape_list has length {len(true_shape_list)}\")"
      ],
      "metadata": {
        "id": "eIdWmHR4TkE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make a prediction**"
      ],
      "metadata": {
        "id": "wRrlimKETuFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to call `predict` and pass in lists; look at the error message\n",
        "try:\n",
        "    detection_model.predict(preprocessed_image_list, true_shape_list)\n",
        "except AttributeError as e:\n",
        "    print(\"Error message:\", e)"
      ],
      "metadata": {
        "id": "uEWs1VINTxGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn a list of tensors into a tensor\n",
        "preprocessed_image_tensor = tf.concat(preprocessed_image_list, axis=0)\n",
        "true_shape_tensor = tf.concat(true_shape_list, axis=0)\n",
        "\n",
        "print(f\"preprocessed_image_tensor shape: {preprocessed_image_tensor.shape}\")\n",
        "print(f\"true_shape_tensor shape: {true_shape_tensor.shape}\")"
      ],
      "metadata": {
        "id": "6eTEWsEtUI_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the images\n",
        "prediction_dict = detection_model.predict(preprocessed_image_tensor, true_shape_tensor)\n",
        "\n",
        "print(\"keys in prediction_dict:\")\n",
        "for key in prediction_dict.keys():\n",
        "    print(key)"
      ],
      "metadata": {
        "id": "EDtj3Y7gUMP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Calculate Loss**"
      ],
      "metadata": {
        "id": "LzLa0JztUUEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    losses_dict = detection_model.loss(prediction_dict, true_shape_tensor)\n",
        "except RuntimeError as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "P5LHUMoGUXAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f578c9-1a87-4242-ff05-45c0fe3aa26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groundtruth tensor boxes has not been provided\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the ground truth bounding boxes\n",
        "gt_boxes_list = gt_box_tensors[0:2]\n",
        "\n",
        "# Get the ground truth class labels\n",
        "gt_classes_list = gt_classes_one_hot_tensors[0:2]\n",
        "\n",
        "# Provide the ground truth to the model\n",
        "detection_model.provide_groundtruth(\n",
        "            groundtruth_boxes_list=gt_boxes_list,\n",
        "            groundtruth_classes_list=gt_classes_list)"
      ],
      "metadata": {
        "id": "lw8RMWPeUgms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the loss after you've provided the ground truth\n",
        "losses_dict = detection_model.loss(prediction_dict, true_shape_tensor)\n",
        "\n",
        "# View the loss dictionary\n",
        "losses_dict = detection_model.loss(prediction_dict, true_shape_tensor)\n",
        "print(f\"loss dictionary keys: {losses_dict.keys()}\")\n",
        "print(f\"localization loss {losses_dict['Loss/localization_loss']:.8f}\")\n",
        "print(f\"classification loss {losses_dict['Loss/classification_loss']:.8f}\")"
      ],
      "metadata": {
        "id": "biQXb9xGUm81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's just reset the model so that you can practice setting it up yourself!\n",
        "detection_model.provide_groundtruth(groundtruth_boxes_list=[], groundtruth_classes_list=[])"
      ],
      "metadata": {
        "id": "T7UXmJAIUrqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the training step**"
      ],
      "metadata": {
        "id": "TVvDKfCuVO36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decorate with @tf.function for faster training (remember, graph mode!)\n",
        "@tf.function\n",
        "def train_step_fn(image_list,\n",
        "                groundtruth_boxes_list,\n",
        "                groundtruth_classes_list,\n",
        "                model,\n",
        "                optimizer,\n",
        "                vars_to_fine_tune):\n",
        "    \"\"\"A single training iteration.\n",
        "\n",
        "    Args:\n",
        "      image_list: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
        "        Note that the height and width can vary across images, as they are\n",
        "        reshaped within this function to be 640x640.\n",
        "      groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
        "        tf.float32 representing groundtruth boxes for each image in the batch.\n",
        "      groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
        "        with type tf.float32 representing groundtruth boxes for each image in\n",
        "        the batch.\n",
        "\n",
        "    Returns:\n",
        "      A scalar tensor representing the total loss for the input batch.\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "    ### START CODE HERE (Replace instances of `None` with your code) ###\n",
        "\n",
        "        # Preprocess the images\n",
        "        preprocessed_image_list=[]\n",
        "        true_shape_list=[]\n",
        "        model.provide_groundtruth(\n",
        "        groundtruth_boxes_list=groundtruth_boxes_list,\n",
        "        groundtruth_classes_list=groundtruth_classes_list)\n",
        "\n",
        "        for img in image_list:\n",
        "          processed_img, true_shape = model.preprocess(img)\n",
        "          preprocessed_image_list.append(processed_img)\n",
        "          true_shape_list.append(true_shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        preprocessed_image_tensor = tf.concat(preprocessed_image_list, axis=0)\n",
        "        true_shape_tensor = tf.concat(true_shape_list, axis=0)\n",
        "\n",
        "        # Make a prediction\n",
        "        prediction_dict = model.predict(preprocessed_image_tensor, true_shape_tensor)\n",
        "\n",
        "        # Calculate the total loss (sum of both losses)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        losses_dict = model.loss(prediction_dict, true_shape_tensor)\n",
        "        total_loss=losses_dict['Loss/localization_loss']+losses_dict['Loss/classification_loss']\n",
        "        # Calculate the gradients\n",
        "        gradients = tape.gradient(total_loss,vars_to_fine_tune)\n",
        "        optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
        "        # Optimize the model's selected variables\n",
        "\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "dc_1ORLyVSE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the Training Loop**"
      ],
      "metadata": {
        "id": "9PM6v4BoVopS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Start fine-tuning!', flush=True)\n",
        "\n",
        "for idx in range(num_batches):\n",
        "    # Grab keys for a random subset of examples\n",
        "    all_keys = list(range(len(train_images_np)))\n",
        "    random.shuffle(all_keys)\n",
        "    example_keys = all_keys[:batch_size]\n",
        "\n",
        "    # Get the ground truth\n",
        "    gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
        "    gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
        "\n",
        "    # get the images\n",
        "    image_tensors = [train_image_tensors[key] for key in example_keys]\n",
        "\n",
        "    # Training step (forward pass + backwards pass)\n",
        "    total_loss = train_step_fn(image_tensors,\n",
        "                               gt_boxes_list,\n",
        "                               gt_classes_list,\n",
        "                               detection_model,\n",
        "                               optimizer,\n",
        "                               to_fine_tune\n",
        "                              )\n",
        "\n",
        "    if idx % 10 == 0:\n",
        "        print('batch ' + str(idx) + ' of ' + str(num_batches)\n",
        "        + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
        "\n",
        "print('Done fine-tuning!')"
      ],
      "metadata": {
        "id": "LZa8GANjVrde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5558a154-2341-4ea4-cde9-e765d39e6ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start fine-tuning!\n",
            "batch 0 of 100, loss=1.5091523\n",
            "batch 10 of 100, loss=1.6050851\n",
            "batch 20 of 100, loss=1.3391234\n",
            "batch 30 of 100, loss=0.82051253\n",
            "batch 40 of 100, loss=0.608968\n",
            "batch 50 of 100, loss=0.5170442\n",
            "batch 60 of 100, loss=0.43395132\n",
            "batch 70 of 100, loss=0.38090706\n",
            "batch 80 of 100, loss=0.3664794\n",
            "batch 90 of 100, loss=0.3494301\n",
            "Done fine-tuning!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess, predict, and post process an image**"
      ],
      "metadata": {
        "id": "Clf7z1sbe31Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Again, uncomment this decorator if you want to run inference eagerly\n",
        "@tf.function\n",
        "def detect(input_tensor):\n",
        "    \"\"\"Run detection on an input image.\n",
        "\n",
        "    Args:\n",
        "    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
        "      Note that height and width can be anything since the image will be\n",
        "      immediately resized according to the needs of the model within this\n",
        "      function.\n",
        "\n",
        "    Returns:\n",
        "    A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
        "      and `detection_scores`).\n",
        "    \"\"\"\n",
        "    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
        "    prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
        "\n",
        "    ### START CODE HERE (Replace instances of `None` with your code) ###\n",
        "    # use the detection model's postprocess() method to get the the final detections\n",
        "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return detections"
      ],
      "metadata": {
        "id": "DHY9SldWe85Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "test_image_dir = '/content/drive/MyDrive/Database/testing_calcium_assay_jpg/videos'\n",
        "items = os.listdir(test_image_dir)\n",
        "for x in range(len(items)):\n",
        "    test_video_path = os.path.join(test_image_dir+'/'+items[x])\n",
        "    test_images_np = []\n",
        "    vid = cv2.VideoCapture(test_video_path)\n",
        "    for i in range (1000):\n",
        "        ret, frame = vid.read() # ret means whether there is a frame or not, frame is nd.array 968x548x3\n",
        "        test_images_np.append(np.expand_dims(frame, axis=0))\n",
        "\n",
        "    # After the loop release the cap object\n",
        "    vid.release()\n",
        "    # Destroy all the windows\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    ##############################################################################\n",
        "    # Note that the first frame will trigger tracing of the tf.function, which will\n",
        "    # take some time, after which inference should be fast.\n",
        "\n",
        "    label_id_offset = 1\n",
        "    box_infos = {'x':[], 'y':[], 'mean_brightness':[],'frame':[],'score':[],'boxes':[]}\n",
        "\n",
        "    for j in range(len(test_images_np)):\n",
        "    # for j in range(0,999,100):\n",
        "        this_frame = test_images_np[j][0,:,:,0]\n",
        "        input_tensor = tf.convert_to_tensor(test_images_np[j], dtype=tf.float32)\n",
        "        detections = detect(input_tensor) # detections in a single frame\n",
        "        # filtered_detections = detections[detections['detection_scores'].numpy()>0.3]\n",
        "        for i in range(len(detections['detection_boxes'][0])): # number of boxes in this frame\n",
        "            if detections['detection_scores'][0][i].numpy() > 0.1:\n",
        "                this_box = detections['detection_boxes'][0][i].numpy() # (y_min,x_min,y_max,x_max)\n",
        "                if (this_box[2]-this_box[0])*(this_box[3]-this_box[1])<0.1: # max area: 0.1 x whole frame\n",
        "                    x_center = int((this_box[1]+this_box[3])/2*968)\n",
        "                    y_center = int((this_box[0]+this_box[2])/2*548)\n",
        "                    box_infos['x'].append(x_center)\n",
        "                    box_infos['y'].append(y_center)\n",
        "                    box_infos['mean_brightness'].append(np.mean(this_frame[y_center-3:y_center+3,x_center-3:x_center+3]))\n",
        "                    box_infos['frame'].append(j)\n",
        "                    box_infos['score'].append(detections['detection_scores'][0][i].numpy())\n",
        "                    box_infos['boxes'].append(this_box)\n",
        "\n",
        "    box_infos_df = pd.DataFrame.from_dict(box_infos)\n",
        "    test_video_results_path = os.path.join('/content/drive/MyDrive/Database/testing_calcium_assay_jpg/video_results/'+items[x] + '.csv')\n",
        "    box_infos_df.to_csv(test_video_results_path)\n",
        "    #######################################################\n",
        "    # Note that the first frame will trigger tracing of the tf.function, which will\n",
        "    # take some time, after which inference should be fast.\n",
        "    label_id_offset = 1\n",
        "    results = {'boxes': [], 'scores': []}\n",
        "\n",
        "    for i in range(0,999,100):\n",
        "    # for i in range(len(test_images_np)):\n",
        "        input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\n",
        "        detections = detect(input_tensor)\n",
        "        plot_detections(\n",
        "          test_images_np[i][0],\n",
        "          detections['detection_boxes'][0].numpy(),\n",
        "          detections['detection_classes'][0].numpy().astype(np.uint32)\n",
        "          + label_id_offset,\n",
        "          detections['detection_scores'][0].numpy(),\n",
        "          category_index, figsize=(15, 20), image_name=\"/content/drive/MyDrive/Database/testing_calcium_assay_jpg/results/\" + items[x] + ('%03d' % i) + \".jpg\")\n",
        "        results['boxes'].append(detections['detection_boxes'][0][0].numpy())\n",
        "        results['scores'].append(detections['detection_scores'][0][0].numpy())"
      ],
      "metadata": {
        "id": "H5--MIc7ovy8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}